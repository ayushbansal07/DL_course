{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This particular assignment focuses on text classification using CNN. It has been picking up pace over the past few years. So, I thought this would be a good exercise to try out. The dataset is provided to you and there will be specific instrucions on how to curate the data, split into train and validation and the like.  You will be using MXnet for this task.  The data comprises tweets pertaining to common causes of cancer. The objective is to classify the tweets as medically relevant or not.  The dataset is skewed with positive class or 'yes' being 6 times less frequent than the negative class or 'no'. (Total marks = 50). Individual marks to the sub-problems are given in bracket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 1298\n",
      "(1506, 109) 7506\n"
     ]
    }
   ],
   "source": [
    "####FOR Logging in Jupyter Notebook#######\n",
    "import logging\n",
    "import sys\n",
    "root_logger = logging.getLogger()\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "root_logger.addHandler(stdout_handler)\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# these are the modules you are allowed to work with. \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import sys, os\n",
    "from numpy import random\n",
    "from collections import Counter, namedtuple\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as fscore\n",
    "import re\n",
    "\n",
    "'''\n",
    "First job is to clean and preprocess the social media text. (5)\n",
    "\n",
    "1) Replace URLs and mentions (i.e strings which are preceeded with @)\n",
    "2) Segment #hastags \n",
    "3) Remove emoticons and other unicode characters\n",
    "'''\n",
    "CLEAN = re.compile(\"[\\s\\n\\r\\t.,:;\\-_\\'\\\"?!#&()\\/%\\[\\]\\{\\}\\<\\>\\\\$@\\!\\*\\+\\=]\")\n",
    "URL = re.compile(\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\")\n",
    "\n",
    "def preprocess_tweet(input_text):\n",
    "    '''\n",
    "    Input: The input string read directly from the file\n",
    "    \n",
    "    Output: Pre-processed tweet text\n",
    "    '''\n",
    "    temp = [x.lower() for x in input_text.split(\" \") if '@' not in x and '#' not in x and not URL.match(x)]\n",
    "    temp = ' '.join(temp)\n",
    "    temp = CLEAN.split(temp)\n",
    "    cleaned_text = ' '.join(temp)    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# read the input file and create the set of positive examples and negative examples. \n",
    "\n",
    "file=open('cancer_data.tsv')\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    text2= preprocess_tweet(line[0]).strip().split()\n",
    "    if line[1]=='yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1]=='no':\n",
    "        neg_data.append(text2)\n",
    "\n",
    "print(len(pos_data), len(neg_data))     \n",
    "\n",
    "sentences= list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels= [1 for _ in pos_data]\n",
    "neg_labels= [0 for _ in neg_data]\n",
    "y=list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y=np.array(y)\n",
    "\n",
    "'''\n",
    "After this you will obtain the following :\n",
    "\n",
    "1) sentences =  List of sentences having the positive and negative examples with all the positive examples first\n",
    "2) y = List of labels with the positive labels first.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Before running the CNN there are a few things one needs to take care of: (5)\n",
    "\n",
    "1) Pad the sentences so that all of them are of the same length\n",
    "2) Build a vocabulary comprising all unique words that occur in the corpus\n",
    "3) Convert each sentence into a corresponding vector by replacing each word in the sentence with the index in the vocabulary. \n",
    "\n",
    "Example :\n",
    "S1 = a b a c\n",
    "S2 = d c a \n",
    "\n",
    "Step 1:  S1= a b a c, \n",
    "         S2 =d c a </s> \n",
    "         (Both sentences are of equal length). \n",
    "\n",
    "Step 2:  voc={a:1, b:2, c:3, d:4, </s>: 5}\n",
    "\n",
    "Step 3:  S1= [1,2,1,3]\n",
    "         S2= [4,3,1,5]\n",
    "\n",
    "'''\n",
    "\n",
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    cnt = Counter({'</s>' : 0})\n",
    "    mx_len = 0\n",
    "    for sent in sentences:\n",
    "        ln = len(sent)\n",
    "        mx_len = max(ln,mx_len)\n",
    "        cnt.update(sent)\n",
    "        #for word in sent:\n",
    "            #if word not in vocabulary:\n",
    "            #    vocabulary[word] = len(vocabulary)\n",
    "    vocabulary = mx.contrib.text.vocab.Vocabulary(cnt)   \n",
    "    word_vectors = []\n",
    "    for sent in sentences:\n",
    "        pad_length = mx_len - len(sent)\n",
    "        padded_sent = sent + ['</s>']*pad_length\n",
    "        word_vectors.append(vocabulary.to_indices(padded_sent))\n",
    "        '''\n",
    "        temp = np.zeros(mx_len)\n",
    "        for i, word in enumerate(sent):\n",
    "            temp[i] = vocabulary.to_indices([word])\n",
    "        word_vectors.append(temp)\n",
    "        '''\n",
    "    word_vectors = np.array(word_vectors) \n",
    "    return word_vectors, vocabulary\n",
    "\n",
    "\n",
    "x, vocabulary = create_word_vectors(sentences)\n",
    "print(x.shape, len(vocabulary))\n",
    "\n",
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with [cpu(0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "/home/ayush/anaconda3/lib/python3.6/site-packages/mxnet/model.py:572: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n",
      "  self.initializer(k, v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Batch [60]\tSpeed: 272.93 samples/sec\taccuracy=0.867500\n",
      "Epoch[0] Resetting Data Iterator\n",
      "Epoch[0] Time cost=4.563\n",
      "Epoch[0] Validation-accuracy=0.915625\n",
      "Epoch[1] Batch [60]\tSpeed: 288.78 samples/sec\taccuracy=0.961667\n",
      "Epoch[1] Resetting Data Iterator\n",
      "Epoch[1] Time cost=4.187\n",
      "Epoch[1] Validation-accuracy=0.909375\n",
      "Epoch[2] Batch [60]\tSpeed: 245.24 samples/sec\taccuracy=0.991667\n",
      "Epoch[2] Resetting Data Iterator\n",
      "Epoch[2] Time cost=4.925\n",
      "Epoch[2] Validation-accuracy=0.900000\n",
      "Epoch[3] Batch [60]\tSpeed: 234.78 samples/sec\taccuracy=0.992500\n",
      "Epoch[3] Resetting Data Iterator\n",
      "Epoch[3] Time cost=5.151\n",
      "Epoch[3] Validation-accuracy=0.903125\n",
      "Epoch[4] Batch [60]\tSpeed: 272.99 samples/sec\taccuracy=0.993333\n",
      "Epoch[4] Resetting Data Iterator\n",
      "Epoch[4] Time cost=4.438\n",
      "Epoch[4] Validation-accuracy=0.903125\n",
      "Epoch[5] Batch [60]\tSpeed: 289.75 samples/sec\taccuracy=0.995833\n",
      "Epoch[5] Resetting Data Iterator\n",
      "Epoch[5] Time cost=4.262\n",
      "Epoch[5] Validation-accuracy=0.906250\n",
      "Epoch[6] Batch [60]\tSpeed: 288.57 samples/sec\taccuracy=0.996667\n",
      "Epoch[6] Resetting Data Iterator\n",
      "Epoch[6] Time cost=4.196\n",
      "Epoch[6] Validation-accuracy=0.903125\n",
      "Epoch[7] Batch [60]\tSpeed: 277.60 samples/sec\taccuracy=0.996667\n",
      "Epoch[7] Resetting Data Iterator\n",
      "Epoch[7] Time cost=4.357\n",
      "Epoch[7] Validation-accuracy=0.900000\n",
      "Epoch[8] Batch [60]\tSpeed: 246.04 samples/sec\taccuracy=0.996667\n",
      "Epoch[8] Resetting Data Iterator\n",
      "Epoch[8] Time cost=4.908\n",
      "Epoch[8] Validation-accuracy=0.896875\n",
      "Epoch[9] Batch [60]\tSpeed: 265.62 samples/sec\taccuracy=0.998333\n",
      "Epoch[9] Resetting Data Iterator\n",
      "Epoch[9] Time cost=4.552\n",
      "Epoch[9] Validation-accuracy=0.896875\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We now define the neural architecture of the CNN. The architecture is defined as : (10)\n",
    "\n",
    "1) Embedding layer that converts the vector representation of the sentence from a one-hot encoding to a fixed sized word embedding\n",
    "   (mx.sym.Embedding)\n",
    "   \n",
    "2) Convolution + activation + max pooling layer \n",
    "   (mx.sym.Convolution+ mx.sym.Activation+ mx.sym.Pooling)\n",
    "   This procedure is to be followed for different sizes of filters (the filters corresponding to size 2 looks at the bigram distribution, 3 looks at trigram etc. \n",
    "\n",
    "3) Concat all the filters together (mx.sym.Concat)\n",
    "\n",
    "4) Pass the results through a fully Connected layer of size 2 and then run softmax on it. \n",
    "   (mx.sym.FullyConnected, mx.sym.SoftmaxOutput)\n",
    "   \n",
    "\n",
    "We then initialize the intermediate layers of appropriate size and train the model using back prop. (10)\n",
    "(Look up the mxnet tutorial if you have any doubt)\n",
    "\n",
    "Run the classifier and for each epoch with a specified batch size observe the accuracy on the training set and test set (5)\n",
    "\n",
    "\n",
    "Default parameters:\n",
    "\n",
    "1) No of epochs = 10\n",
    "2) Batch size = 20\n",
    "3) Size of word embeddings = 200\n",
    "4) Size of filters =[2,3,4,5]\n",
    "5) Filter embedding= 100\n",
    "6) Optimizer = rmsprop\n",
    "7) learning rate = 0.005\n",
    "\n",
    "'''\n",
    "\n",
    "def create_model(vocab_size,max_time,out_dim=2,embedding_dim=200,batch_size=20,weight_matrix=None):\n",
    "    input_data = mx.sym.Variable('data')\n",
    "    output_labels = mx.sym.Variable('softmax_label')\n",
    "    input_embed = mx.sym.Embedding(data=input_data,input_dim=vocab_size,output_dim=embedding_dim,\\\n",
    "                                   name='embed_weights')\n",
    "    conv_inp = mx.sym.Reshape(data=input_embed, shape=(-1,1,max_time,embedding_dim))\n",
    "    filter_sizes = [2,3,4,5]\n",
    "    num_filters = 100\n",
    "    conv_outs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        out = mx.sym.Convolution(data=conv_inp,kernel=(filter_size,embedding_dim),num_filter=num_filters)\n",
    "        out = mx.sym.Activation(data=out,act_type='relu')\n",
    "        out = mx.sym.Pooling(data=out,pool_type='max',kernel=(max_time - filter_size + 1,1))\n",
    "        conv_outs.append(out)\n",
    "        \n",
    "    all_outs = mx.sym.Concat(*conv_outs,dim=1)\n",
    "    all_outs = mx.sym.Reshape(data=all_outs,shape=(-1,len(filter_sizes)*num_filters))\n",
    "    scores = mx.sym.FullyConnected(data=all_outs,num_hidden=out_dim)\n",
    "    probs = mx.sym.SoftmaxOutput(data=scores,name='softmax')\n",
    "    \n",
    "    #Initialiazation : Unfiorm(0.1)\n",
    "    arg_params = {}\n",
    "    if weight_matrix is not None:\n",
    "        arg_params={'embed_weights' : weight_matrix}\n",
    "    model = mx.model.FeedForward(probs,optimizer='rmsprop',num_epoch=10,learning_rate=0.005,\\\n",
    "                                 numpy_batch_size=batch_size,initializer=mx.initializer.Uniform(0.1),\\\n",
    "                                 arg_params=arg_params)\n",
    "    return model\n",
    "\n",
    "MAX_TIME = x_train.shape[1]\n",
    "model = create_model(len(vocabulary),MAX_TIME)\n",
    "model.fit(X=x_train,y=y_train,batch_end_callback = mx.callback.Speedometer(20,60),eval_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy on test Set :  0.8940397350993378\n",
      "F1 score for Positive class :  0.4074074074074074\n",
      "F1 score for Negative class :  0.9418181818181818\n"
     ]
    }
   ],
   "source": [
    "pred_probs = model.predict(x_test)\n",
    "preds = pred_probs.argmax(axis=1)\n",
    "acc = np.sum(preds==y_test)/len(y_test)\n",
    "print(\"Final Accuracy on test Set : \",acc)\n",
    "fs = fscore(y_test,preds)\n",
    "print(\"F1 score for Positive class : \",fs[2][1])\n",
    "print(\"F1 score for Negative class : \",fs[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSo far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \\n\\nThe final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \\n\\n1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\\n   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\\n   \\n2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \\n   Experiment with different hyper-paramters to show the performance in terms of metric? \\n   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\\n    \\n\\nDelivearbles:\\n\\nThe ipython notebook with the results to each part of the question. \\n\\n\\nP.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \\nHappy coding \\n\\nRitam Dutt\\n14CS30041\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \n",
    "\n",
    "The final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \n",
    "\n",
    "1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\n",
    "   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\n",
    "   \n",
    "2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \n",
    "   Experiment with different hyper-paramters to show the performance in terms of metric? \n",
    "   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\n",
    "    \n",
    "\n",
    "Delivearbles:\n",
    "\n",
    "The ipython notebook with the results to each part of the question. \n",
    "\n",
    "\n",
    "P.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \n",
    "Happy coding \n",
    "\n",
    "Ritam Dutt\n",
    "14CS30041\n",
    "\n",
    "'''\n",
    "\n",
    "#See next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using fastText 300-dimensional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained token embedding vectors from ./fasttext/wiki.simple.vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/mxnet/contrib/text/embedding.py:278: UserWarning: At line 1 of the pre-trained text embedding file: token 111051 with 1-dimensional vector [300.0] is likely a header and is skipped.\n",
      "  'skipped.' % (line_num, token, elems))\n"
     ]
    }
   ],
   "source": [
    "embeddings = mx.contrib.text.embedding.FastText(embedding_root='./',vocabulary=vocabulary)\n",
    "all_tokens = vocabulary.to_tokens(list(range(len(vocabulary))))\n",
    "weight_matrix = embeddings.get_vecs_by_tokens(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with [cpu(0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "/home/ayush/anaconda3/lib/python3.6/site-packages/mxnet/model.py:572: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n",
      "  self.initializer(k, v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Batch [60]\tSpeed: 199.13 samples/sec\taccuracy=0.873333\n",
      "Epoch[0] Resetting Data Iterator\n",
      "Epoch[0] Time cost=6.247\n",
      "Epoch[0] Validation-accuracy=0.903125\n",
      "Epoch[1] Batch [60]\tSpeed: 207.44 samples/sec\taccuracy=0.967500\n",
      "Epoch[1] Resetting Data Iterator\n",
      "Epoch[1] Time cost=5.837\n",
      "Epoch[1] Validation-accuracy=0.903125\n",
      "Epoch[2] Batch [60]\tSpeed: 207.87 samples/sec\taccuracy=0.985000\n",
      "Epoch[2] Resetting Data Iterator\n",
      "Epoch[2] Time cost=5.818\n",
      "Epoch[2] Validation-accuracy=0.893750\n",
      "Epoch[3] Batch [60]\tSpeed: 200.60 samples/sec\taccuracy=0.992500\n",
      "Epoch[3] Resetting Data Iterator\n",
      "Epoch[3] Time cost=6.028\n",
      "Epoch[3] Validation-accuracy=0.900000\n",
      "Epoch[4] Batch [60]\tSpeed: 159.52 samples/sec\taccuracy=0.995000\n",
      "Epoch[4] Resetting Data Iterator\n",
      "Epoch[4] Time cost=7.577\n",
      "Epoch[4] Validation-accuracy=0.903125\n",
      "Epoch[5] Batch [60]\tSpeed: 170.91 samples/sec\taccuracy=0.996667\n",
      "Epoch[5] Resetting Data Iterator\n",
      "Epoch[5] Time cost=7.182\n",
      "Epoch[5] Validation-accuracy=0.906250\n",
      "Epoch[6] Batch [60]\tSpeed: 183.96 samples/sec\taccuracy=0.996667\n",
      "Epoch[6] Resetting Data Iterator\n",
      "Epoch[6] Time cost=6.575\n",
      "Epoch[6] Validation-accuracy=0.900000\n",
      "Epoch[7] Batch [60]\tSpeed: 175.74 samples/sec\taccuracy=0.997500\n",
      "Epoch[7] Resetting Data Iterator\n",
      "Epoch[7] Time cost=6.873\n",
      "Epoch[7] Validation-accuracy=0.909375\n",
      "Epoch[8] Batch [60]\tSpeed: 208.31 samples/sec\taccuracy=0.998333\n",
      "Epoch[8] Resetting Data Iterator\n",
      "Epoch[8] Time cost=5.807\n",
      "Epoch[8] Validation-accuracy=0.896875\n",
      "Epoch[9] Batch [60]\tSpeed: 209.56 samples/sec\taccuracy=0.998333\n",
      "Epoch[9] Resetting Data Iterator\n",
      "Epoch[9] Time cost=5.771\n",
      "Epoch[9] Validation-accuracy=0.903125\n"
     ]
    }
   ],
   "source": [
    "MAX_TIME = x_train.shape[1]\n",
    "model2 = create_model(len(vocabulary),MAX_TIME,embedding_dim=weight_matrix.shape[1],weight_matrix=weight_matrix)\n",
    "model2.fit(X=x_train,y=y_train,batch_end_callback = mx.callback.Speedometer(20,60),eval_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing using Fasttext Embeddings\n",
      "Final Accuracy on test Set :  0.8973509933774835\n",
      "F1 score for Positive class :  0.43636363636363634\n",
      "F1 score for Negative class :  0.9435336976320582\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing using Fasttext Embeddings\")\n",
    "pred_probs = model2.predict(x_test)\n",
    "preds = pred_probs.argmax(axis=1)\n",
    "acc = np.sum(preds==y_test)/len(y_test)\n",
    "print(\"Final Accuracy on test Set : \",acc)\n",
    "fs = fscore(y_test,preds)\n",
    "print(\"F1 score for Positive class : \",fs[2][1])\n",
    "print(\"F1 score for Negative class : \",fs[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using Glove 200-dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained token embedding vectors from ./glove/glove.6B.200d.txt\n"
     ]
    }
   ],
   "source": [
    "del weight_matrix\n",
    "EMBEDDINGS_FILE = 'glove.6B.200d.txt'\n",
    "embeddings = mx.contrib.text.embedding.GloVe(EMBEDDINGS_FILE,embedding_root='./',vocabulary=vocabulary)\n",
    "all_tokens = vocabulary.to_tokens(list(range(len(vocabulary))))\n",
    "weight_matrix = embeddings.get_vecs_by_tokens(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with [cpu(0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "/home/ayush/anaconda3/lib/python3.6/site-packages/mxnet/model.py:572: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n",
      "  self.initializer(k, v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0] Batch [60]\tSpeed: 241.06 samples/sec\taccuracy=0.861667\n",
      "Epoch[0] Resetting Data Iterator\n",
      "Epoch[0] Time cost=5.143\n",
      "Epoch[0] Validation-accuracy=0.871875\n",
      "Epoch[1] Batch [60]\tSpeed: 243.90 samples/sec\taccuracy=0.968333\n",
      "Epoch[1] Resetting Data Iterator\n",
      "Epoch[1] Time cost=4.951\n",
      "Epoch[1] Validation-accuracy=0.887500\n",
      "Epoch[2] Batch [60]\tSpeed: 273.01 samples/sec\taccuracy=0.992500\n",
      "Epoch[2] Resetting Data Iterator\n",
      "Epoch[2] Time cost=4.430\n",
      "Epoch[2] Validation-accuracy=0.890625\n",
      "Epoch[3] Batch [60]\tSpeed: 268.31 samples/sec\taccuracy=0.992500\n",
      "Epoch[3] Resetting Data Iterator\n",
      "Epoch[3] Time cost=4.503\n",
      "Epoch[3] Validation-accuracy=0.893750\n",
      "Epoch[4] Batch [60]\tSpeed: 213.13 samples/sec\taccuracy=0.994167\n",
      "Epoch[4] Resetting Data Iterator\n",
      "Epoch[4] Time cost=5.668\n",
      "Epoch[4] Validation-accuracy=0.884375\n",
      "Epoch[5] Batch [60]\tSpeed: 205.69 samples/sec\taccuracy=0.998333\n",
      "Epoch[5] Resetting Data Iterator\n",
      "Epoch[5] Time cost=5.977\n",
      "Epoch[5] Validation-accuracy=0.887500\n",
      "Epoch[6] Batch [60]\tSpeed: 199.15 samples/sec\taccuracy=0.996667\n",
      "Epoch[6] Resetting Data Iterator\n",
      "Epoch[6] Time cost=6.057\n",
      "Epoch[6] Validation-accuracy=0.896875\n",
      "Epoch[7] Batch [60]\tSpeed: 187.18 samples/sec\taccuracy=0.997500\n",
      "Epoch[7] Resetting Data Iterator\n",
      "Epoch[7] Time cost=6.445\n",
      "Epoch[7] Validation-accuracy=0.893750\n",
      "Epoch[8] Batch [60]\tSpeed: 180.66 samples/sec\taccuracy=0.997500\n",
      "Epoch[8] Resetting Data Iterator\n",
      "Epoch[8] Time cost=6.687\n",
      "Epoch[8] Validation-accuracy=0.896875\n",
      "Epoch[9] Batch [60]\tSpeed: 180.65 samples/sec\taccuracy=0.997500\n",
      "Epoch[9] Resetting Data Iterator\n",
      "Epoch[9] Time cost=6.680\n",
      "Epoch[9] Validation-accuracy=0.890625\n"
     ]
    }
   ],
   "source": [
    "MAX_TIME = x_train.shape[1]\n",
    "model3 = create_model(len(vocabulary),MAX_TIME,embedding_dim=weight_matrix.shape[1],weight_matrix=weight_matrix)\n",
    "model3.fit(X=x_train,y=y_train,batch_end_callback = mx.callback.Speedometer(20,60),eval_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing using Glove Embeddings\n",
      "Final Accuracy on test Set :  0.8973509933774835\n",
      "F1 score for Positive class :  0.43636363636363634\n",
      "F1 score for Negative class :  0.9435336976320582\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing using Glove Embeddings\")\n",
    "pred_probs = model3.predict(x_test)\n",
    "preds = pred_probs.argmax(axis=1)\n",
    "acc = np.sum(preds==y_test)/len(y_test)\n",
    "print(\"Final Accuracy on test Set : \",acc)\n",
    "fs = fscore(y_test,preds)\n",
    "print(\"F1 score for Positive class : \",fs[2][1])\n",
    "print(\"F1 score for Negative class : \",fs[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results of Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Other metrics</b> - F1 score can be a good metric for skewed datasets. But since our dataset is highly skewed, we might even consider <b>Micro F1 Score</b> for the positive class only. We have displayed both the metrics (accuracy, micro F1 for Positive class) for all our experiments<br>\n",
    "<b>Experiments</b><br>\n",
    "<b>1. Naive model (Random Initialisation of Emebddings)</b> - <br>\n",
    "Final Accuracy on test Set :  0.8973509933774835<br>\n",
    "F1 score for Positive class :  0.43636363636363634<br>\n",
    "F1 score for Negative class :  0.9435336976320582<br>\n",
    "<b>2. Fasttext Embeddings (300 dimensional)</b> - <br>\n",
    "Final Accuracy on test Set :  0.9006622516556292<br>\n",
    "F1 score for Positive class :  0.46428571428571436<br>\n",
    "F1 score for Negative class :  0.9452554744525548<br>\n",
    "The accuracy and micro f1 both improved for 300 dimensional fasttext embeddings<br>\n",
    "<b>3. Glove Embeddings (200 dimensional)</b> - <br>\n",
    "Final Accuracy on test Set :  0.9105960264900662<br>\n",
    "F1 score for Positive class :  0.5573770491803278<br>\n",
    "F1 score for Negative class :  0.9502762430939227<br>\n",
    "The use of 200 dimensional Glove embeddings has given best performance on test set in terms of both Accuracy and micro F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
