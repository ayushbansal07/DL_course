{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This particular assignment focuses on text classification using CNN. It has been picking up pace over the past few years. So, I thought this would be a good exercise to try out. The dataset is provided to you and there will be specific instrucions on how to curate the data, split into train and validation and the like.  You will be using MXnet for this task.  The data comprises tweets pertaining to common causes of cancer. The objective is to classify the tweets as medically relevant or not.  The dataset is skewed with positive class or 'yes' being 6 times less frequent than the negative class or 'no'. (Total marks = 50). Individual marks to the sub-problems are given in bracket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 1298\n",
      "(1506, 122) 15070\n"
     ]
    }
   ],
   "source": [
    "# these are the modules you are allowed to work with. \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import sys, os\n",
    "from numpy import random\n",
    "from collections import Counter, namedtuple\n",
    "\n",
    "'''\n",
    "First job is to clean and preprocess the social media text. (5)\n",
    "\n",
    "1) Replace URLs and mentions (i.e strings which are preceeded with @)\n",
    "2) Segment #hastags \n",
    "3) Remove emoticons and other unicode characters\n",
    "'''\n",
    "\n",
    "def preprocess_tweet(input_text):\n",
    "    '''\n",
    "    Input: The input string read directly from the file\n",
    "    \n",
    "    Output: Pre-processed tweet text\n",
    "    '''\n",
    "    cleaned_text = input_text\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# read the input file and create the set of positive examples and negative examples. \n",
    "\n",
    "file=open('cancer_data.tsv')\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    text2= preprocess_tweet(line[0]).strip().split()\n",
    "    if line[1]=='yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1]=='no':\n",
    "        neg_data.append(text2)\n",
    "\n",
    "print(len(pos_data), len(neg_data))     \n",
    "\n",
    "sentences= list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels= [1 for _ in pos_data]\n",
    "neg_labels= [0 for _ in neg_data]\n",
    "y=list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y=np.array(y)\n",
    "\n",
    "'''\n",
    "After this you will obtain the following :\n",
    "\n",
    "1) sentences =  List of sentences having the positive and negative examples with all the positive examples first\n",
    "2) y = List of labels with the positive labels first.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Before running the CNN there are a few things one needs to take care of: (5)\n",
    "\n",
    "1) Pad the sentences so that all of them are of the same length\n",
    "2) Build a vocabulary comprising all unique words that occur in the corpus\n",
    "3) Convert each sentence into a corresponding vector by replacing each word in the sentence with the index in the vocabulary. \n",
    "\n",
    "Example :\n",
    "S1 = a b a c\n",
    "S2 = d c a \n",
    "\n",
    "Step 1:  S1= a b a c, \n",
    "         S2 =d c a </s> \n",
    "         (Both sentences are of equal length). \n",
    "\n",
    "Step 2:  voc={a:1, b:2, c:3, d:4, </s>: 5}\n",
    "\n",
    "Step 3:  S1= [1,2,1,3]\n",
    "         S2= [4,3,1,5]\n",
    "\n",
    "'''\n",
    "\n",
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    cnt = Counter({'</s>' : 0})\n",
    "    mx_len = 0\n",
    "    for sent in sentences:\n",
    "        ln = len(sent)\n",
    "        mx_len = max(ln,mx_len)\n",
    "        cnt.update(sent)\n",
    "        #for word in sent:\n",
    "            #if word not in vocabulary:\n",
    "            #    vocabulary[word] = len(vocabulary)\n",
    "    vocabulary = mx.contrib.text.vocab.Vocabulary(cnt)   \n",
    "    word_vectors = []\n",
    "    for sent in sentences:\n",
    "        pad_length = mx_len - len(sent)\n",
    "        padded_sent = sent + ['</s>']*pad_length\n",
    "        word_vectors.append(vocabulary.to_indices(padded_sent))\n",
    "        '''\n",
    "        temp = np.zeros(mx_len)\n",
    "        for i, word in enumerate(sent):\n",
    "            temp[i] = vocabulary.to_indices([word])\n",
    "        word_vectors.append(temp)\n",
    "        '''\n",
    "    word_vectors = np.array(word_vectors) \n",
    "    return word_vectors, vocabulary\n",
    "\n",
    "\n",
    "x, vocabulary = create_word_vectors(sentences)\n",
    "print(x.shape, len(vocabulary))\n",
    "\n",
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "/home/ayush/anaconda3/lib/python3.6/site-packages/mxnet/model.py:572: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n",
      "  self.initializer(k, v)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We now define the neural architecture of the CNN. The architecture is defined as : (10)\n",
    "\n",
    "1) Embedding layer that converts the vector representation of the sentence from a one-hot encoding to a fixed sized word embedding\n",
    "   (mx.sym.Embedding)\n",
    "   \n",
    "2) Convolution + activation + max pooling layer \n",
    "   (mx.sym.Convolution+ mx.sym.Activation+ mx.sym.Pooling)\n",
    "   This procedure is to be followed for different sizes of filters (the filters corresponding to size 2 looks at the bigram distribution, 3 looks at trigram etc. \n",
    "\n",
    "3) Concat all the filters together (mx.sym.Concat)\n",
    "\n",
    "4) Pass the results through a fully Connected layer of size 2 and then run softmax on it. \n",
    "   (mx.sym.FullyConnected, mx.sym.SoftmaxOutput)\n",
    "   \n",
    "\n",
    "We then initialize the intermediate layers of appropriate size and train the model using back prop. (10)\n",
    "(Look up the mxnet tutorial if you have any doubt)\n",
    "\n",
    "Run the classifier and for each epoch with a specified batch size observe the accuracy on the training set and test set (5)\n",
    "\n",
    "\n",
    "Default parameters:\n",
    "\n",
    "1) No of epochs = 10\n",
    "2) Batch size = 20\n",
    "3) Size of word embeddings = 200\n",
    "4) Size of filters =[2,3,4,5]\n",
    "5) Filter embedding= 100\n",
    "6) Optimizer = rmsprop\n",
    "7) learning rate = 0.005\n",
    "\n",
    "'''\n",
    "#EMBEDDINGS_FILE = 'glove.6B.200d.txt'\n",
    "\n",
    "#embeddings = mx.contrib.text.embedding.GloVe(EMBEDDINGS_FILE,embedding_root='./',vocabulary=vocabulary)\n",
    "def create_model(vocab_size,max_time,out_dim=2,embedding_dim=200,batch_size=20):\n",
    "    input_data = mx.sym.Variable('data')\n",
    "    output_labels = mx.sym.Variable('softmax_label')\n",
    "    input_embed = mx.sym.Embedding(data=input_data,input_dim=vocab_size,output_dim=embedding_dim)\n",
    "    conv_inp = mx.sym.Reshape(data=input_embed, shape=(-1,1,max_time,embedding_dim))\n",
    "    filter_sizes = [2,3,4,5]\n",
    "    num_filters = 100\n",
    "    conv_outs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        out = mx.sym.Convolution(data=conv_inp,kernel=(filter_size,embedding_dim),num_filter=num_filters)\n",
    "        out = mx.sym.Activation(data=out,act_type='relu')\n",
    "        out = mx.sym.Pooling(data=out,pool_type='max',kernel=(max_time - filter_size + 1,1))\n",
    "        conv_outs.append(out)\n",
    "        \n",
    "    all_outs = mx.sym.Concat(*conv_outs,dim=1)\n",
    "    all_outs = mx.sym.Reshape(data=all_outs,shape=(-1,len(filter_sizes)*num_filters))\n",
    "    scores = mx.sym.FullyConnected(data=all_outs,num_hidden=out_dim)\n",
    "    probs = mx.sym.SoftmaxOutput(data=scores,name='softmax')\n",
    "    \n",
    "    model = mx.model.FeedForward(probs,optimizer='rmsprop',num_epoch=10,learning_rate=0.005)\n",
    "    return model\n",
    "MAX_TIME = x_train.shape[1]\n",
    "model = create_model(len(vocabulary),MAX_TIME)\n",
    "model.fit(X=x_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(vocab_size,max_time,out_dim=2,embedding_dim=200,batch_size=20):\n",
    "    input_data = mx.sym.Variable('input')\n",
    "    output_labels = mx.sym.Variable('output')\n",
    "    input_embed = mx.sym.Embedding(data=input_data,input_dim=vocab_size,output_dim=embedding_dim)\n",
    "    conv_inp = mx.sym.Reshape(data=input_embed, shape=(-1,1,max_time,embedding_dim))\n",
    "    filter_sizes = [2,3,4,5]\n",
    "    num_filters = 100\n",
    "    conv_outs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        out = mx.sym.Convolution(data=conv_inp,kernel=(filter_size,embedding_dim),num_filter=num_filters)\n",
    "        out = mx.sym.Activation(data=out,act_type='relu')\n",
    "        out = mx.sym.Pooling(data=out,pool_type='max',kernel=(max_time - filter_size + 1,1))\n",
    "        conv_outs.append(out)\n",
    "        \n",
    "    all_outs = mx.sym.Concat(*conv_outs,dim=1)\n",
    "    all_outs = mx.sym.Reshape(data=all_outs,shape=(-1,len(filter_sizes)*num_filters))\n",
    "    scores = mx.sym.FullyConnected(data=all_outs,num_hidden=out_dim)\n",
    "    probs = mx.sym.SoftmaxOutput(data=scores,label=output_labels)\n",
    "    \n",
    "    cnn = probs\n",
    "    return cnn\n",
    "\n",
    "\n",
    "ctx = mx.cpu()\n",
    "BATCH_SIZE = 20\n",
    "MAX_TIME = x_train.shape[1]\n",
    "cnn = create_model(len(vocabulary),MAX_TIME,out_dim=2, embedding_dim=200,batch_size=BATCH_SIZE)\n",
    "CNNModel = namedtuple('CNNModel',['cnn_exec','symbol','input','label','param_blocks'])\n",
    "input_shapes = {}\n",
    "input_shapes['input'] = (BATCH_SIZE,MAX_TIME)\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_names = cnn.list_arguments()\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name not in ['input', 'output']:\n",
    "        args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx,args=arg_arrays,args_grad=args_grad,grad_req='add')\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name not in ['input', 'output']:\n",
    "        initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "        param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "input_data = cnn_exec.arg_dict['input']\n",
    "output_label = cnn_exec.arg_dict['output']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, input=input_data, label=output_label, param_blocks=param_blocks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSo far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \\n\\nThe final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \\n\\n1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\\n   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\\n   \\n2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \\n   Experiment with different hyper-paramters to show the performance in terms of metric? \\n   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\\n    \\n\\nDelivearbles:\\n\\nThe ipython notebook with the results to each part of the question. \\n\\n\\nP.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \\nHappy coding \\n\\nRitam Dutt\\n14CS30041\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So far, the assignment has been posed in a manner so that you can refer to directly the mxnet tutorial on the same problem. \n",
    "\n",
    "The final 15 marks is meant to carry out experimentations of your own and observe how the results change by experimentation. \n",
    "\n",
    "1) Would the results improve if instead of using the word embeddings that is based solely on frequency, if you have been able to incorporate sub-word information\n",
    "   (In short run fasttext on the corpus and use the word embeddings generated by fastetxt). (8)\n",
    "   \n",
    "2) Accuracy might not be the best way to measure the performance of a skewed dataset. What other metrics would you use ? Why? \n",
    "   Experiment with different hyper-paramters to show the performance in terms of metric? \n",
    "   You can assume that we want to identify all the medically relevant tweets (i.e. tweets with 'yes' class more). (7)\n",
    "    \n",
    "\n",
    "Delivearbles:\n",
    "\n",
    "The ipython notebook with the results to each part of the question. \n",
    "\n",
    "\n",
    "P.S: This assignment is part of a research question I am working on my free time. So if you have any insights, I'd love to hear them. \n",
    "Happy coding \n",
    "\n",
    "Ritam Dutt\n",
    "14CS30041\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As',\n",
       " 'people’s',\n",
       " 'anger',\n",
       " 'boils',\n",
       " 'over',\n",
       " 'in',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'there',\n",
       " 'are',\n",
       " 'talks',\n",
       " 'that',\n",
       " 'the',\n",
       " 'Rouhani',\n",
       " 'government',\n",
       " 'should',\n",
       " 'resign.',\n",
       " 'But',\n",
       " 'you',\n",
       " 'do',\n",
       " 'not',\n",
       " 'cure',\n",
       " 'cancer',\n",
       " 'with',\n",
       " 'an',\n",
       " 'aspirin.',\n",
       " 'The',\n",
       " 'tumor',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'be',\n",
       " 'removed-',\n",
       " '-',\n",
       " 'and',\n",
       " 'tumor',\n",
       " 'is',\n",
       " 'IRI',\n",
       " 'and',\n",
       " 'the',\n",
       " 'surgeons',\n",
       " 'are',\n",
       " 'the',\n",
       " 'people.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
