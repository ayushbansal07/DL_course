{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ayush/anaconda3/lib/python3.6/site-packages/scipy/stats/morestats.py:12: DeprecationWarning: Importing from numpy.testing.decorators is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing.decorators import setastest\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-2-1f4c32b8126e>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "  CLEANING_PATTERN = re.compile(\"[\\s\\n\\r\\t.,:;\\-_\\'\\\"?!#&()\\/%\\[\\]\\{\\}\\<\\>\\\\$@\\!\\*\\+\\=]\")\n"
     ]
    }
   ],
   "source": [
    "CLEANING_PATTERN = re.compile(\"[\\s\\n\\r\\t.,:;\\-_\\'\\\"?!#&()\\/%\\[\\]\\{\\}\\<\\>\\\\$@\\!\\*\\+\\=]\")\n",
    "VOCAB_SIZE = 5800\n",
    "MAX_TIME = 15\n",
    "LSTM_HIDDEN = 256\n",
    "MAX_NORM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "with open('train.txt') as f:\n",
    "    for line in f:\n",
    "        x_train.append(line)\n",
    "\n",
    "with open('test.txt') as f:\n",
    "    for line in f:\n",
    "        x_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = tokenizer.texts_to_sequences(x_train)\n",
    "test_x = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_lenghts_train = [len(sent) for sent in train_x]\n",
    "sent_lenghts_test = [len(sent) for sent in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pad_sequences(train_x,maxlen=MAX_TIME,padding='post')\n",
    "test_x = pad_sequences(test_x,maxlen=MAX_TIME,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.zeros((len(train_x),MAX_TIME))\n",
    "test_y = np.zeros((len(test_x),MAX_TIME))\n",
    "train_y[:,:-1] = train_x[:,1:]\n",
    "test_y[:,:-1] = test_x[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = mx.nd.array(train_x)\n",
    "train_y = mx.nd.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordIndex = tokenizer.word_index\n",
    "wI = {}\n",
    "for k,v in wordIndex.items():\n",
    "    if v < VOCAB_SIZE:\n",
    "        wI[k] = v\n",
    "wordIndex = wI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = mx.contrib.text.vocab.Vocabulary(Counter(tokenizer.word_counts))\n",
    "my_indexing = [0]\n",
    "for i in range(1,VOCAB_SIZE):\n",
    "    myword = tokenizer.index_word[i]\n",
    "    my_indexing.append(vocabulary.token_to_idx[myword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILE = 'glove.6B.200d.txt'\n",
    "embeddings = mx.contrib.text.embedding.GloVe(EMBEDDINGS_FILE,embedding_root='./',vocabulary=vocabulary)\n",
    "all_tokens = vocabulary.to_tokens(list(range(len(vocabulary))))\n",
    "weight_matrix = embeddings.get_vecs_by_tokens(all_tokens)\n",
    "weight_matrix = weight_matrix[my_indexing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _clip_by_global_norm(_module,max_norm):\n",
    "        assert _module.binded and _module.params_initialized \\\n",
    "               and _module.optimizer_initialized\n",
    "        grad_array = []\n",
    "        for grad in _module._exec_group.grad_arrays:\n",
    "            grad_array += grad\n",
    "        return mx.gluon.utils.clip_global_norm(grad_array, max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = VOCAB_SIZE\n",
    "embedding_dim = 200\n",
    "input_data = mx.sym.Variable('data')\n",
    "label = mx.sym.Variable('softmax_label')\n",
    "input_embed = mx.sym.Embedding(data=input_data,input_dim=vocab_size,output_dim=embedding_dim,\\\n",
    "                                   name='embed')\n",
    "lstm_cell = mx.rnn.LSTMCell(num_hidden=LSTM_HIDDEN)\n",
    "begin_state = lstm_cell.begin_state()\n",
    "output, states = lstm_cell.unroll(MAX_TIME,input_embed, begin_state,merge_outputs=True)\n",
    "pred = mx.sym.Reshape(output, shape=(-1, LSTM_HIDDEN))\n",
    "pred = mx.sym.FullyConnected(data=pred, num_hidden=vocab_size, name='pred')\n",
    "pred = mx.sym.Reshape(pred, shape=(-1, vocab_size))\n",
    "\n",
    "label = mx.sym.Reshape(label, shape=(-1,))\n",
    "loss = mx.sym.SoftmaxOutput(data=pred,label=label)\n",
    "#logits = mx.sym.log_softmax(pred, axis=-1)\n",
    "#loss = -mx.sym.pick(logits, label, axis=-1, keepdims=True)\n",
    "#loss = mx.sym.mean(loss, axis=0, exclude=True)\n",
    "#loss = mx.sym.make_loss(loss, name='nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "model = mx.module.Module(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bind(data_shapes=[('data',(batch_size,MAX_TIME))], label_shapes=[('softmax_label', (batch_size,MAX_TIME))])\n",
    "model.init_params(initializer=mx.initializer.Uniform(0.1))\n",
    "model.set_params({'embed_weight' : weight_matrix},{},allow_missing=True)\n",
    "model.init_optimizer('rmsprop',optimizer_params=(('learning_rate', 0.005), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train Loss : 243.871240\n",
      "Epoch 2 : Train Loss : 171.824470\n",
      "Epoch 3 : Train Loss : 166.982176\n",
      "Epoch 4 : Train Loss : 163.159878\n",
      "Epoch 5 : Train Loss : 160.079741\n",
      "Epoch 6 : Train Loss : 157.727996\n",
      "Epoch 7 : Train Loss : 155.909802\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-3974434d6f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0m_clip_by_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_NORM\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mMAX_TIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_multi_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-612445814827>\u001b[0m in \u001b[0;36m_clip_by_global_norm\u001b[0;34m(_module, max_norm)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_arrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mgrad_array\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mxnet/gluon/utils.py\u001b[0m in \u001b[0;36mclip_global_norm\u001b[0;34m(arrays, max_norm)\u001b[0m\n\u001b[1;32m    121\u001b[0m     total_norm = ndarray.add_n(*[ndarray.dot(x, x).as_in_context(ctx)\n\u001b[1;32m    122\u001b[0m                                  for x in (arr.reshape((-1,)) for arr in arrays)])\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         warnings.warn(UserWarning('nan or inf is detected. Clipping results will be undefined.'),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1874\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1877\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "num_batches = int(len(train_x)/batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    eval_metric = mx.metric.CrossEntropy()\n",
    "    for i in range(num_batches):\n",
    "        bt = mx.io.DataBatch(data=[train_x[i*batch_size:(i+1)*batch_size]], label=[train_y[i*batch_size:(i+1)*batch_size]])\n",
    "        model.forward(bt)\n",
    "        model.backward()\n",
    "        _clip_by_global_norm(model,MAX_NORM*batch_size*MAX_TIME)\n",
    "        model.update()\n",
    "        outputs = model.get_outputs(merge_multi_context=True)\n",
    "        eval_metric.update(train_y[i*batch_size:(i+1)*batch_size].reshape(-1,),outputs[0])\n",
    "        train_loss += eval_metric.get()[1]\n",
    "    print(\"Epoch %d : Train Loss : %f\"%(epoch+1,train_loss))\n",
    "    model.save_checkpoint(\"model\",10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for last word\n",
    "\n",
    "test_accuracy_1 = 0\n",
    "num_batches = int(len(test_x)/batch_size)\n",
    "for i in range(num_batches):\n",
    "    bt = mx.io.DataBatch(data=[train_x[i*batch_size:(i+1)*batch_size]], label=[train_y[i*batch_size:(i+1)*batch_size]])\n",
    "    model.forward(bt)\n",
    "    test_outs = model.get_outputs(merge_multi_context=True)[0].asnumpy()\n",
    "    test_outs = test_outs.reshape((batch_size,MAX_TIME,-1))\n",
    "    test_preds = np.argmax(test_outs,axis=-1)\n",
    "    batch_y = np.array(test_y[i*batch_size:(i+1)*batch_size],dtype='int64')\n",
    "    idxing = range(batch_size),np.array(sent_lenghts_test[i*batch_size:(i+1)*batch_size])-1\n",
    "    acc = np.sum(test_preds[idxing] == batch_y[idxing])\n",
    "    test_accuracy_1 += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for Task 1 :  0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy for Task 1 : \",test_accuracy_1/(num_batches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
