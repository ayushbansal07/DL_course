{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing multiple linear regression using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030 8\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Name : Ayush Bansal\n",
    "Roll No: 15CS30006\n",
    "\n",
    "Assignment 1a\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "You will not import any other library other than these provided.\n",
    "\n",
    "We provide the concrete_dataset as an example.\n",
    "There are 8 dependent variables columns(1-8).\n",
    "The last column (concrete compressive strength) is the value we wish to estimate.\n",
    "'''\n",
    "\n",
    "df= pd.read_csv('Concrete_Data.csv')\n",
    "df.head()\n",
    "\n",
    "# reads the file and stores in 2 numpy arrays.\n",
    "# X has the input features and Y has the output value in numpy array\n",
    "\n",
    "X = df.iloc[:,:-1].values\n",
    "Y = df.iloc[:,-1].values\n",
    "\n",
    "rows,cols= X.shape[0], X.shape[1] \n",
    "# how to get the number of rows and columns in the dataset.\n",
    "# Rows correspond to the number of input instances, columns correspond to the feature of an input\n",
    "\n",
    "print(rows,cols)\n",
    "\n",
    "np.random.seed(42) # to ensure that the same seed is generated\n",
    "\n",
    "# write code to shuffle the dataset\n",
    "\n",
    "def shuffle_dataset(X,Y):\n",
    "    shuffle_state = np.random.get_state()\n",
    "    np.random.shuffle(X)\n",
    "    np.random.set_state(shuffle_state)\n",
    "    np.random.shuffle(Y)\n",
    "    \n",
    "    '''\n",
    "        Write code to shuffle the dataset here. \n",
    "        \n",
    "        Args: \n",
    "            X: Input feature ndarray\n",
    "            Y: Input values ndarray\n",
    "            \n",
    "        Return:\n",
    "            X and Y shuffled in place\n",
    "    \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "shuffle_dataset (X,Y) #This line should be added to shuffle the dataset\n",
    "training_size = int(0.8*rows)\n",
    "X_train = X[:training_size]\n",
    "y_train = Y[:training_size]\n",
    "X_test = X[training_size:]\n",
    "y_test = Y[training_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Linear Regression class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self):\n",
    "        #Initialize all parameters\n",
    "        \n",
    "        self.w = np.random.uniform(-1,1,cols) #? Sample an array corresponding to the number of input features (cols) from a uniform distribution between -1 and 1\n",
    "        self.b = np.random.uniform(-1,1) #? Sample from a uniform distribution between -1 and 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            Do a forward pass of the classifier:\n",
    "            Args:\n",
    "                x: Input X matrix\n",
    "            Return:\n",
    "                y: y = X.w + b  \n",
    "        '''\n",
    "        # Complete this function \n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        return x.dot(self.w) + self.b\n",
    "        \n",
    "    \n",
    "    def backward(self, x, ypred, y_train, lr):\n",
    "        '''\n",
    "            Computes all gradients and updates the parameters w and b\n",
    "            Args:\n",
    "                x : x\n",
    "                ypred: y=wx+b\n",
    "                y_train = ground truth values\n",
    "                lr = learning rate\n",
    "        '''\n",
    "        # Complete this function\n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        #print(x.shape,ypred.shape,y_train.shape)\n",
    "        grad_w = x*np.reshape((ypred - y_train),(-1,1))\n",
    "        #print(grad_w.shape)\n",
    "        grad_w = np.mean(grad_w,axis=0)\n",
    "        #print(grad_w.shape)\n",
    "        grad_b = np.mean(ypred - y_train)\n",
    "        self.w -= lr*grad_w\n",
    "        self.b -= lr*grad_b\n",
    "\n",
    "def MSELoss(y, ypred):\n",
    "    '''\n",
    "        Args:\n",
    "            y: ground truth labels\n",
    "            ypred: predicted labels\n",
    "        Return:\n",
    "            Mean squared error loss\n",
    "    '''\n",
    "    \n",
    "    # Compute the mean squared error \n",
    "    #raise NotImplementedError\n",
    "    return np.mean(0.5*(np.square(ypred - y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Gradient Descent\n",
      "Loss fuction decrease after 10000 epochs of training\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIwwHSdIIw0GSNMJwkCSN+P+tBoRQaA1VaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss\n",
      "79.55651752556727\n",
      "Starting to test\n",
      "Final test loss: 72.72755170276626\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of epochs as well as the learning rate. \n",
    "# Keep the values fixed.\n",
    "\n",
    "print('Starting Training with Gradient Descent')\n",
    "lreg = LinearRegression()\n",
    "epochs = 10000\n",
    "learning_rate = 0.0000001\n",
    "\n",
    "loss_history = []\n",
    "epoch_history = []\n",
    "\n",
    "# Gradient Descent\n",
    "for e in range(epochs):\n",
    "    ypred = lreg.forward(X_train) # computes the predicted values\n",
    "    loss = MSELoss(y_train, ypred) # computes the MSE loss between the actual and predicted values\n",
    "    # store the values of loss per epoch\n",
    "    if e==0 or (e+1)%100==0:\n",
    "        loss_history.append(loss)\n",
    "        epoch_history.append(e+1)\n",
    "        \n",
    "    \n",
    "    lreg.backward(X_train, ypred, y_train, learning_rate)\n",
    "\n",
    "print('Loss fuction decrease after ' + str(epochs) + ' epochs of training')\n",
    "#Plot the decrease in loss with epoch\n",
    "plt.plot(epoch_history, loss_history)\n",
    "plt.show()\n",
    "\n",
    "print('Final training loss')   \n",
    "ypred = lreg.forward(X_train)\n",
    "y_train_loss= MSELoss(y_train, ypred) # Print training loss ?\n",
    "print(y_train_loss)\n",
    "print('Starting to test')\n",
    "ytest_pred= lreg.forward(X_test) # find predictions on test set ?\n",
    "loss= MSELoss(y_test,ytest_pred) # compute loss on test set ?\n",
    "print('Final test loss: ' + str(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
